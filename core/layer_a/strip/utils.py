import re
import unicodedata

# The below corpus was enhanced using AI
# suspicious character detection for prompt injection attacks
SUSPICIOUS_CHARS_RE = re.compile(
    r"[\u200B\u200C\u200D\u200E\u200F"  # Zero-width & directional markers
    r"\u202A-\u202E"                    # LRE, RLE, PDF, LRO, RLO
    r"\u2066-\u2069"                    # LRI, RLI, FSI, PDI
    r"\uFEFF"                           # BOM
    r"\u00A0"                           # Non-breaking space
    r"\u1680"                           # Ogham space mark
    r"\u2000-\u200A"                    # En quad through hair space
    r"\u2028\u2029"                     # Line/paragraph separator
    r"\u202F"                           # Narrow no-break space
    r"\u205F"                           # Medium mathematical space
    r"\u3000"                           # Ideographic space
    r"\uFFF9-\uFFFB]"                   # Interlinear annotation chars
)

# Homoglyph detection patterns - common substitutions in attacks
HOMOGLYPH_PATTERNS = {
    # Latin lookalikes
    'Ğ°': 'a',  # Cyrillic 'a'
    'Ğµ': 'e',  # Cyrillic 'e'  
    'Ğ¾': 'o',  # Cyrillic 'o'
    'Ñ€': 'p',  # Cyrillic 'p'
    'Ñ': 'c',  # Cyrillic 'c'
    'Ñƒ': 'y',  # Cyrillic 'y'
    'Ñ…': 'x',  # Cyrillic 'x'
    'Ñ–': 'i',  # Cyrillic 'i'
    'Ñ˜': 'j',  # Cyrillic 'j'
    'Ñ•': 's',  # Cyrillic 's'
    
    # Greek lookalikes
    'Î±': 'a',  # Greek alpha
    'Î²': 'B',  # Greek beta (looks like B)
    'Î³': 'y',  # Greek gamma
    'Îµ': 'e',  # Greek epsilon
    'Î¿': 'o',  # Greek omicron
    'Ï': 'p',  # Greek rho
    'Ï„': 't',  # Greek tau (looks like t)
    'Ï…': 'u',  # Greek upsilon
    'Ï‡': 'x',  # Greek chi
    
    # Mathematical and other lookalikes
    'â„“': 'l',  # Script small l
    'ğ“': 'l',  # Mathematical script small l
    'ğ¥': 'l',  # Mathematical bold small l
    'ğ‘™': 'l',  # Mathematical italic small l
    'â¸»': '-',  # Two-em dash (looks like hyphen)
    'â€“': '-',  # En dash
    'â€”': '-',  # Em dash
}

# Control character detection
CONTROL_CHARS_RE = re.compile(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]')

# Suspicious Unicode categories that might hide attacks
SUSPICIOUS_CATEGORIES = {
    'Cf',  # Other, format (invisible formatting)
    'Mn',  # Mark, nonspacing (combining chars that modify appearance)
    'Me',  # Mark, enclosing (combining chars that enclose)
}

def detect_homoglyphs(text: str, normalize: bool = True):
    """
    Detect and optionally normalize homoglyph characters that could be used in attacks.
    
    Args:
        text: Input text to analyze
        normalize: If True, replace homoglyphs with ASCII equivalents
        
    Returns:
        tuple: (normalized_text, metadata)
    """
    homoglyphs_found = []
    normalized_text = text

    
    for i, char in enumerate(text):
        if char in HOMOGLYPH_PATTERNS:
            unicode_name = unicodedata.name(char, 'UNKNOWN')
            homoglyphs_found.append({
                'char': char,
                'position': i,
                'replacement': HOMOGLYPH_PATTERNS[char],
                'unicode_name': unicode_name
            })
            
            if normalize:
                normalized_text = normalized_text.replace(char, HOMOGLYPH_PATTERNS[char])
    
    metadata = {
        'homoglyphs_found': homoglyphs_found,
        'homoglyph_count': len(homoglyphs_found),
        'was_normalized': normalized_text != text
    }
    return normalized_text, metadata

# def detect_suspicious_unicode_categories(text: str):
#     """
#     Detect characters in suspicious Unicode categories that could hide attacks.
#     """
#     suspicious_chars = []
    
#     for i, char in enumerate(text):
#         category = unicodedata.category(char)
#         if category in SUSPICIOUS_CATEGORIES:
#             suspicious_chars.append({
#                 'char': char,
#                 'position': i,
#                 'category': category,
#                 'unicode_name': unicodedata.name(char, 'UNKNOWN')
#             })
    
#     return suspicious_chars

def detect_control_characters(text: str, strip_controls: bool = True):
    """
    Detect and optionally strip control characters.
    
    Args:
        text: Input text to analyze
        strip_controls: If True, remove control characters
        
    Returns:
        tuple: (cleaned_text, metadata)
    """
    control_matches = list(CONTROL_CHARS_RE.finditer(text))
    control_chars = []
    
    for match in control_matches:
        char = match.group()
        control_chars.append({
            'char': repr(char),  # Safe representation
            'position': match.start(),
            'ord': ord(char),
            'description': f'Control character 0x{ord(char):02X}'
        })
    
    cleaned_text = CONTROL_CHARS_RE.sub('', text) if strip_controls else text
    
    return cleaned_text, {
        'control_chars': control_chars,
        'control_count': len(control_chars),
        'was_cleaned': cleaned_text != text
    }

# #TODO: Evaluation required
# def analyze_text_structure(text: str):
#     """
#     Analyze text structure for potential injection patterns.
#     """
#     analysis = {
#         'length': len(text),
#         'line_count': text.count('\n') + 1,
#         'word_count': len(text.split()),
#         'char_counts': {},
#         #'suspicious_patterns': []
#     }
    
#     # Character frequency analysis
#     for char in text:
#         category = unicodedata.category(char)
#         analysis['char_counts'][category] = analysis['char_counts'].get(category, 0) + 1
        
#     #TODO: Evaluate whether to include this. Prompts should be allowed to be long. Overly long propts would be blocked anyway?

#     # # Look for suspicious patterns
#     # if text.count('\n') > 50:
#     #     analysis['suspicious_patterns'].append('excessive_newlines')
    
#     # if len(text) > 10000:
#     #     analysis['suspicious_patterns'].append('excessive_length')
    
#     # Look for repeated suspicious chars
#     for char in ['\u200B', '\u200C', '\u200D', '\uFEFF']:
#         if text.count(char) > 5:
#             analysis['suspicious_patterns'].append(f'repeated_{unicodedata.name(char, "unknown").lower()}')
    
#     return analysis